@article{elhage2021mathematical,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year={2021},
  journal={Transformer Circuits Thread},
  note={\url{https://transformer-circuits.pub/2021/framework/index.html}},
  urldate={2025-05-13}
}

@article{olsson2022context,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year={2022},
  journal={Transformer Circuits Thread},
  note={\url{https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}},
  urldate={2025-05-13}
}

@misc{brown2020languagemodelsfewshotlearners,
  title={Language Models are Few-Shot Learners}, 
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year={2020},
  eprint={2005.14165},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2005.14165}
}

@misc{wu2024languagemodelsplanahead,
  title={Do language models plan ahead for future tokens?}, 
  author={Wilson Wu and John X. Morris and Lionel Levine},
  year={2024},
  eprint={2404.00859},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2404.00859}
}

@misc{wendler2024llamasworkenglishlatent,
  title={Do Llamas Work in English? On the Latent Language of Multilingual Transformers}, 
  author={Chris Wendler and Veniamin Veselovsky and Giovanni Monea and Robert West},
  year={2024},
  eprint={2402.10588},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2402.10588}
}

@inproceedings{dumas2024how,
  title={How do {Llama}s process multilingual text? {A} latent exploration through activation patching},
  author={Cl{\'e}ment Dumas and Veniamin Veselovsky and Giovanni Monea and Robert West and Chris Wendler},
  booktitle={{ICML} 2024 Workshop on Mechanistic Interpretability},
  year={2024},
  url={https://openreview.net/forum?id=0ku2hIm4BS}
}

@misc{fierro2025multilinguallanguagemodelsremember,
  title={How Do Multilingual Language Models Remember Facts?}, 
  author={Constanza Fierro and Negar Foroutan and Desmond Elliott and Anders S{\o}gaard},
  year={2025},
  eprint={2410.14387},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2410.14387}
}

@misc{kantamneni2025languagemodelsusetrigonometry,
  title={Language Models Use Trigonometry to Do Addition}, 
  author={Subhash Kantamneni and Max Tegmark},
  year={2025},
  eprint={2502.00873},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2502.00873}
}

@misc{nikankin2024arithmeticalgorithmslanguagemodels,
  title={Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics}, 
  author={Yaniv Nikankin and Anja Reusch and Aaron Mueller and Yonatan Belinkov},
  year={2024},
  eprint={2410.21272},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2410.21272}
}

@article{lindsey2025biology,
  author={Lindsey, Jack and Gurnee, Wes and Ameisen, Emmanuel and Chen, Brian and Pearce, Adam and Turner, Nicholas L. and Citro, Craig and Abrahams, David and Carter, Shan and Hosmer, Basil and Marcus, Jonathan and Sklar, Michael and Templeton, Adly and Bricken, Trenton and McDougall, Callum and Cunningham, Hoagy and Henighan, Thomas and Jermyn, Adam and Jones, Andy and Persic, Andrew and Qi, Zhenyi and Thompson, T. Ben and Zimmerman, Sam and Rivoire, Kelley and Conerly, Thomas and Olah, Chris and Batson, Joshua},
  title={On the Biology of a Large Language Model},
  journal={Transformer Circuits Thread},
  year={2025},
  url={https://transformer-circuits.pub/2025/attribution-graphs/biology.html},
  urldate={2025-05-13}
}

@misc{amodei2025urgency,
  author = {Dario Amodei},
  title = {The Urgency of Interpretability},
  year = {2025},
  url = {https://www.darioamodei.com/post/the-urgency-of-interpretability#top},
  note = {Accessed: 2025-05-13},
  urldate={2025-05-13}
}

@misc{dutta2024thinkstepbystepmechanisticunderstanding,
  title={How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning}, 
  author={Subhabrata Dutta and Joykirat Singh and Soumen Chakrabarti and Tanmoy Chakraborty},
  year={2024},
  eprint={2402.18312},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2402.18312}
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
  title={{DeepSeek-R1}: Incentivizing Reasoning Capability in {LLM}s via Reinforcement Learning}, 
  author={{DeepSeek-AI} and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
  year={2025},
  eprint={2501.12948},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2501.12948}
}

@misc{nanda2023factfinding, 
  title={Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level}, 
  url={https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall}, 
  journal={Alignment Forum}, 
  author={Nanda, Neel and Rajamanoharan, Senthooran and Kram\’ar, J\’anos and Shah, Rohin}, 
  year={2023}, 
  month={Dec},
  urldate={2025-05-13}
} 

@misc{qwen2025qwen25technicalreport,
  title={{Qwen2.5} Technical Report}, 
  author={{Qwen Team} and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
  year={2025}, % Assuming this is a placeholder for an upcoming report for Qwen2.5 series
  eprint={2412.15115}, % This eprint seems to be for a future date, adjust if a current Qwen2.5 report exists
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2412.15115} 
}

@misc{dai2025sgrpoearlyexitreinforcement,
  title={{S-GRPO}: Early Exit via Reinforcement Learning in Reasoning Models}, 
  author={Muzhi Dai and Chenxu Yang and Qingyi Si},
  year={2025},
  eprint={2505.07686},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2505.07686}
}

@misc{arc2024,
  author       = {{ARC Prize}},
  title        = {{ARC-AGI} Benchmark},
  year         = {2024},
  howpublished = {\url{https://arcprize.org/arc-agi}},
  note         = {Accessed: 2025-05-13},
  urldate      = {2025-05-13}
}

@misc{microsoft2023phi2,
  author = {Javaheripi, Mojan and Bubeck, S{\'{e}}bastien and Eldan, Ronen and Gopi, Sivakanth and Gunasekar, Suriya and Kauffmann, Piero and Lee, Yin Tat and Li, Yuanzhi and Nguyen, Anh and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital Santacroce, Michael and Zhang, Yi and Abdin, Marah and Aneja, Jyoti and Mendes, Caio and Chen, Weizhu and Giorno, Allie}, % Corrected author list order from blog
  year = {2023},
  month = {12},
  title = {Phi-2: The surprising power of small language models},
  howpublished = {Microsoft Research Blog},
  url = {https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/},
  urldate = {2025-05-13}
}

@misc{srivastava2025reasoningabilitysmalllanguage,
  title={Towards Reasoning Ability of Small Language Models}, 
  author={Gaurav Srivastava and Shuxiang Cao and Xuan Wang},
  year={2025},
  eprint={2502.11569},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2502.11569}
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year={2023}, % Original arXiv 2022, NeurIPS 2023
  eprint={2201.11903},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2201.11903}
}

@misc{zhang2024bestpracticesactivationpatching,
  title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods}, 
  author={Fred Zhang and Neel Nanda},
  year={2024}, % Original arXiv 2023
  eprint={2309.16042},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2309.16042}
}

@misc{ribeiro2016whyitrustyou,
  title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
  author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  year={2016},
  eprint={1602.04938},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1602.04938}
}

% References added from the initial draft, not present in user's bib
@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016},
  url={https://arxiv.org/abs/1610.01644}
}

@inproceedings{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2015}, % arXiv 2014, ICLR 2015
  eprint={1409.0473},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/1409.0473}
}

@article{chollet2019measure,
  title={On the measure of intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019},
  url={https://arxiv.org/abs/1911.01547}
}

@article{doshivelez2017rigorous,
  title={Towards A Rigorous Science of Interpretable Machine Learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017},
  url={https://arxiv.org/abs/1702.08608}
}

@inproceedings{geiger2021examining,
  title={Examining Attributions on Textual Data},
  author={Geiger, Atticus and Wu, Zhengxuan and Rozner, Nadav and Lu, Hanson and Singh, Sameer and Chen, Joseph},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={1260--1275},
  year={2021},
  url={https://aclanthology.org/2021.findings-acl.108}
}

@misc{google2024gemma,
  author = {{Google}},
  title = {Gemma: Open Models Based on Gemini Research and Technology},
  year = {2024},
  howpublished = {\url{https://ai.google.dev/gemma}},
  urldate = {2025-05-13}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D.},
  booktitle={Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019},
  url={https://aclanthology.org/N19-1419}
}

@article{ichter2023can,
  title={Can Large Language Models Reason and Plan?},
  author={Ichter, Brian and Kaplan, Lior and Abbasi, Davood and Abbott, Robert and Abసీఆనంద్, Kumar and Adi, Yossi and Agarwal, Shubham and Andreas, Jacob and Anil, Cem and Antonoglou, Ioannis and others},
  journal={arXiv preprint arXiv:2310.09176},
  year={2023},
  url={https://arxiv.org/abs/2310.09176}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in neural information processing systems (NIPS)},
  volume={30},
  year={2017},
  url={https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html}
}

@misc{nanda2022transformerlens,
  author = {Nanda, Neel},
  title = {{TransformerLens (formerly EasyTransformer)}},
  year = {2022},
  howpublished = {GitHub repository},
  note = {\url{https://github.com/neelnanda-io/TransformerLens}},
  urldate = {2025-05-13}
}

@article{nanda2023progress,
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  title={Progress on Interpreting Transformers},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023},
  url={https://arxiv.org/abs/2305.01610}
}

@article{olah2020zoom,
  title={Zoom In: An Introduction to Circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  year={2020},
  volume={5},
  number={3},
  url={https://distill.pub/2020/circuits/zoom-in},
  doi={10.23915/distill.00024.001}
}

@inproceedings{sundararajan2017axiomatic,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning (ICML)},
  volume={70},
  pages={3319--3328},
  year={2017},
  url={http://proceedings.mlr.press/v70/sundararajan17a.html}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems (NIPS)},
  volume={30},
  year={2017},
  url={https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@inproceedings{vig2019analyzing,
  title={Analyzing the Structure of Attention in a Transformer Language Model},
  author={Vig, Jesse and Belinkov, Yonatan},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={63--76},
  year={2019},
  url={https://aclanthology.org/W19-4808}
}

@article{vig2020causal,
  title={Causal mediation analysis for interpreting neural {NLP}: The case of gender bias},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={arXiv preprint arXiv:2004.12990},
  year={2020},
  url={https://arxiv.org/abs/2004.12990}
}

@inproceedings{wolf2020transformers,
  title={{Transformers: State-of-the-Art Natural Language Processing}},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Charles and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={38--45},
  year={2020},
  url={https://www.aclweb.org/anthology/2020.emnlp-demos.6}
}

@article{xu2023comprehensive,
  title={A Comprehensive Study of Large Language Models on Challenging Benchmarks: ARC, HellaSwag, and MMLU},
  author={Xu, Yushi and Chen, Jiaxin and Huang, Xin and Dong, Yuang and Li, Shuo and Tian, Yaoshuai and Liu, Zhedong and Jiang, Junfeng and Zhang, Chen},
  journal={arXiv preprint arXiv:2307.08031},
  year={2023},
  url={https://arxiv.org/abs/2307.08031}
}

    @article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@misc{yang2024qwen2technicalreport,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}

@misc{qwen3_blog,
  author       = {Qwen Team},
  title        = {Qwen3: Think Deeper, Act Faster},
  year         = {2025},
  url          = {https://qwenlm.github.io/blog/qwen3/},
  note         = {Accessed: 2025-05-13}
}
